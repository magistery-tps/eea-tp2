---
title: "Enfoque Estadistico del Aprendizaje - Trabajo Final - Regresion Bayesiana"
fig_width: 3 
fig_height: 3 
output:
  html_document:
    highlight: pygments
    theme: sandstone
    toc: yes
    df_print: paged
    includes:
      before_body: ./header.html
  html_notebook: 
    toc: yes
    toc_float: yes
    df_print: paged
---

### Adrian Marino

### Claudio Collado


## Referencias:

* [Making Predictions from Stan models in R](https://medium.com/@alex.pavlakis/making-predictions-from-stan-models-in-r-3e349dfac1ed)
* [How to represent a categorical predictor rstan?](https://stackoverflow.com/questions/29183577/how-to-represent-a-categorical-predictor-rstan)

```{r}
set.seed(42) 
```


## 1. Librerias

A continuación se realiza la llamada de las librerías a ser utilizadas a lo largo del Trabajo Practico:

```{r message=FALSE, warning=FALSE}
# install.packages(pacman)
# install.packages("https://cran.r-project.org/src/contrib/rstan_2.21.2.tar.gz",repos = NULL,type="source")
# sudo apt-get install libglpk-dev
```

```{r message=FALSE, warning=FALSE}
library(pacman)
p_load(tidyverse, tidymodels, rsample, rstan, shinystan, rstanarm, devtools)
p_load_gh('adrianmarino/commons')

import('../src/dataset.R')
import('../src/plot.R')
import('../src/model.R')
```

## 2. Dataset y Analisis Exploratorio

### 2.1 Lectura del dataset

[Palmer Penguins](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-28/readme.md)


```{r message=FALSE, warning=FALSE}
# Lectura
dataset <- load_dataset() %>%
  mutate_if(is.character, as.factor)

# Resumen
dataset %>% glimpse()
```


```{r}
show_values(dataset %>% select_if(negate(is.numeric)))
```

### 2.2 Variables

Se enumeran y describen breve-mente cada variable que forma parte del dataset:

Varaibles Numéricas:
* bill_length_mm
* bill_depth_mm
* flipper_length_mm
* body_mass_g
* year

Variables Categóricas:
* species
* sex
* island


### 2.3 Conteo de NA

```{r message=FALSE,warning=FALSE}
missings_summary(dataset)
```

### 2.4 Varibles numericas

```{r fig.align='center', fig.height=3, fig.width=5, message=FALSE, warning=FALSE, fig.align='center'}
hist_plots(dataset)
```

```{r}
box_plots(dataset)
```



```{r, fig.show='hide'}
outliers(dataset, column='bill_length_mm')
```


```{r, fig.show='hide'}
outliers(dataset, column='bill_length_mm')
```


```{r, fig.show='hide'}
outliers(dataset, column='bill_depth_mm')
```

```{r, fig.show='hide'}
outliers(dataset, column='flipper_length_mm')
```

```{r, fig.show='hide'}
outliers(dataset, column='body_mass_g')
```


```{r, fig.show='hide'}
outliers(dataset, column='year')
```




```{r fig.align='center', fig.height=4, fig.width=4, message=FALSE, warning=FALSE, fig.align='center'}
bar_plots(dataset)
```

### 2.6 Excluir observaciones con missings

```{r}
dataset <- dataset %>% drop_na()
missings_summary(dataset)
```

### 2.7 Correlaciones

```{r}
corr_plot(dataset)
```

```{r fig.align='center', fig.height=12, fig.width=12, message=FALSE, warning=FALSE, fig.align='center'}
segmented_pairs_plot(dataset, segment_column='species')
```


## 3. Split train - test


```{r message=FALSE, warning=FALSE}
train_test <- train_test_split(dataset, train_size = 0.7, shuffle = TRUE)
train_set <- train_test[[1]]
test_set  <- train_test[[2]]
```

## 4. Modelo lineal

```{r}
lineal_model <- lm(
  body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
  data = train_set
)

summary(lineal_model)
```

## 5. Modelo bayesiano

```{r message=FALSE, warning=FALSE}
model_definition <- "
  data {
    int<lower=1>               obs_count;
    vector<lower=1>[obs_count] x1;
    vector<lower=1>[obs_count] x2;
    vector<lower=1>[obs_count] x3;
    vector[obs_count]          y;
  }
  parameters {
    real          beta0;
    real          beta1;
    real          beta2;
    real          beta3;
    real<lower=0> sigma;
  }
  model {
    beta0 ~ normal(0, 8000);
    beta1 ~ normal(0, 100);
    beta2 ~ normal(0, 100);
    beta3 ~ normal(0, 100);
    sigma ~ exponential(0.1);
  
    y ~ normal(beta0 + beta1 * x1 + beta2 * x2 + beta3 * x3, sigma);
  }
"

options(mc.cores = 24)

bayesion_model <- stan(
  model_code = model_definition,
  data = list(
      obs_count = nrow(train_set),
      y  = colvalues(train_set, 'body_mass_g'),
      x1 = colvalues(train_set, 'bill_length_mm'),
      x2 = colvalues(train_set, 'bill_depth_mm'),
      x3 = colvalues(train_set, 'flipper_length_mm')
  ),
  chains = 30,
  iter   = 10000,
  warmup = 5000,
  thin   = 1
)
```


```{r message=FALSE, warning=FALSE}
params <- c('beta0', 'beta1', 'beta2', 'beta3', 'sigma')

traceplot(bayesion_model, pars = params, inc_warmup = TRUE)
```


```{r}
# launch_shinystan(bayesion_model, rstudio = FALSE)
```


Coeficientes de la regresión múltiple:


```{r}
lineal_model$coefficients
```

Coeficientes descubiertos por la regresión múltiple bayesiana:

```{r}
for(param in params) print(get_posterior_mean(bayesion_model, par=param)[4])
```


```{r}

models_validation(
  lineal_model, 
  bayesion_model, 
  params, 
  test_set,
  vars = c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm') 
)
```

## 6. Modelo lineal con variables categoricas


```{r}
lineal_model_2 <- lm(
    body_mass_g 
      ~ bill_length_mm
      + bill_depth_mm
      + flipper_length_mm
      + sex,

  data = train_set
)

summary(lineal_model_2)
```

## 7. Modelo bayesiano con variables categoricas

Antes que anda transformamos las columnas categoricas a un onehot-encoding:

```{r}
cat_cols      <- c('sex')

train_set_cat <- train_set %>% dummify(cat_cols)
test_set_cat  <- test_set  %>% dummify(cat_cols)

train_set_cat
```


```{r}
to_model_input <- function(df) {
  model.matrix(
    body_mass_g 
      ~ bill_length_mm
      + bill_depth_mm
      + flipper_length_mm
      + sex_female
      + sex_male,

    data = df
  )
}

train_X <- train_set_cat %>% to_model_input()
test_X  <- test_set_cat %>% to_model_input()
```


```{r message=FALSE, warning=FALSE}
model_definition <- "
  data {
    int<lower=1>                 obs_count;
    int<lower=1>                 coef_count;
    matrix[obs_count,coef_count] X;
    vector[obs_count]            y;
  }
  parameters {
    vector[coef_count]  beta;
    real<lower=0>       sigma;
  }
  model {
    beta[1] ~ normal(0, 2000);
    
    beta[2] ~ normal(0, 30);
    beta[3] ~ normal(0, 100);
    beta[4] ~ normal(0, 100);

    beta[5] ~ normal(0, 100);
    beta[6] ~ normal(0, 100);

    sigma ~ exponential(0.1);

    y ~ normal(X * beta, sigma);
  }
"

options(mc.cores = 24)

bayesion_model_2 <- stan(
  model_code = model_definition,
  data = list(
      obs_count  = dim(train_X)[1],
      coef_count = dim(train_X)[2],
      y          = colvalues(train_set_cat, 'body_mass_g'),
      X          = train_X
  ),
  chains = 30,
  iter   = 15000,
  warmup = 5000,
  thin   = 1
)
```


```{r}
params <- c('beta[1]', 'beta[2]', 'beta[3]', 'beta[4]', 'beta[5]', 'beta[6]', 'sigma')

traceplot(bayesion_model_2, inc_warmup = TRUE, pars = params)
```


```{r}
# launch_shinystan(bayesion_model_2, rstudio = FALSE)
```


Coeficientes de la regresión múltiple:


```{r}
lineal_model_2$coefficients
```

Coeficientes descubiertos por la regresión múltiple bayesiana:

```{r}
for(param in params) print(get_posterior_mean(bayesion_model_2, par=param)[4])
```

```{r}
models_validation(
  lineal_model_2, 
  bayesion_model_2, 
  params, 
  test_set, 
  test_set_cat,
  vars = c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'sex_female','sex_male')
)
```


