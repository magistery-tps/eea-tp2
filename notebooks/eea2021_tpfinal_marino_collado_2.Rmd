---
title: "Enfoque Estadistico del Aprendizaje - Trabajo Final - Regresion Bayesiana"
fig_width: 3 
fig_height: 3 
output:
  html_document:
    highlight: pygments
    theme: sandstone
    toc: yes
    df_print: paged
    includes:
      before_body: ./header.html
  html_notebook: 
    toc: yes
    toc_float: yes
    df_print: paged
---

### Adrian Marino

### Claudio Collado


## Referencias:

* [Making Predictions from Stan models in R](https://medium.com/@alex.pavlakis/making-predictions-from-stan-models-in-r-3e349dfac1ed)
* [How to represent a categorical predictor rstan?](https://stackoverflow.com/questions/29183577/how-to-represent-a-categorical-predictor-rstan)

```{r}
set.seed(42) 
```


## 0. Librerias

A continuación se realiza la llamada de las librerías a ser utilizadas a lo largo del Trabajo Practico:

```{r message=FALSE, warning=FALSE}
# install.packages(pacman)
# install.packages("https://cran.r-project.org/src/contrib/rstan_2.21.2.tar.gz",repos = NULL,type="source")
# sudo apt-get install libglpk-dev
```

```{r message=FALSE, warning=FALSE}
library(pacman)
p_load(tidyverse, tidymodels, rsample, rstan, shinystan, rstanarm)
p_load_gh('adrianmarino/commons')

import('../src/dataset.R')
import('../src/plot.R')
import('../src/model.R')
```

## 1. Dataset y Analisis Exploratorio

### 1.1 Lectura del dataset

[Palmer Penguins](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-07-28/readme.md)


```{r message=FALSE, warning=FALSE}
# Lectura
dataset <- load_dataset() %>%
  mutate_if(is.character, as.factor)

# Resumen
dataset %>% glimpse()
```


```{r}
show_values(dataset %>% select_if(negate(is.numeric)))
```

### 1.2 Variables

Se enumeran y describen breve-mente cada variable que forma parte del dataset:

Varaibles Numéricas:
* bill_length_mm
* bill_depth_mm
* flipper_length_mm
* body_mass_g
* year

Variables Categóricas:
* species
* sex
* island


### 1.4 Conteo de NA

```{r message=FALSE,warning=FALSE}
missings_summary(dataset)
```

### 1.5 Varibles numericas

```{r}
hist_plots(dataset)
```

```{r}
box_plots(dataset)
```



```{r, fig.show='hide'}
outliers(dataset, column='bill_length_mm')
```


```{r, fig.show='hide'}
outliers(dataset, column='bill_length_mm')
```


```{r, fig.show='hide'}
outliers(dataset, column='bill_depth_mm')
```

```{r, fig.show='hide'}
outliers(dataset, column='flipper_length_mm')
```

```{r, fig.show='hide'}
outliers(dataset, column='body_mass_g')
```


```{r, fig.show='hide'}
outliers(dataset, column='year')
```




```{r}
bar_plots(dataset)
```


### 1.5 Correlaciones

```{r}
dataset <- dataset %>% drop_na()
missings_summary(dataset)
```

```{r}
corr_plot(dataset)
```

```{r fig.align='center', fig.height=12, fig.width=12, message=FALSE, warning=FALSE, fig.align='center'}
segmented_pairs_plot(dataset, segment_column='species')
```



## 2. Split train - test


```{r message=FALSE, warning=FALSE}
train_test <- train_test_split(dataset, train_size = 0.7, shuffle = TRUE)
train_set <- train_test[[1]]
test_set  <- train_test[[2]]
```

## 3. Modelo lineal

```{r}
lineal_model <- lm(
  body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
  data = train_set
)

summary(lineal_model)
# coefficients_summary(lineal_model)
# anova_summary(lineal_model)
```

## 4. Modelo bayesiano

```{r message=FALSE, warning=FALSE}
model_definition <- "
  data {
    int<lower=1>               obs_count;
    vector<lower=1>[obs_count] x1;
    vector<lower=1>[obs_count] x2;
    vector<lower=1>[obs_count] x3;
    vector[obs_count]          y;
  }
  parameters {
    real          beta0;
    real          beta1;
    real          beta2;
    real          beta3;
    real<lower=0> sigma;
  }
  model {
    beta0 ~ normal(0, 8000);
    beta1 ~ normal(0, 100);
    beta2 ~ normal(0, 100);
    beta3 ~ normal(0, 100);
    sigma ~ exponential(0.1);
  
    y ~ normal(beta0 + beta1 * x1 + beta2 * x2 + beta3 * x3, sigma);
  }
"

options(mc.cores = 24)

bayesion_model <- stan(
  model_code = model_definition,
  data = list(
      obs_count = nrow(train_set),
      y  = colvalues(train_set, 'body_mass_g'),
      x1 = colvalues(train_set, 'bill_length_mm'),
      x2 = colvalues(train_set, 'bill_depth_mm'),
      x3 = colvalues(train_set, 'flipper_length_mm')
  ),
  chains = 30,
  iter   = 10000,
  warmup = 5000,
  thin   = 1
)
```


```{r message=FALSE, warning=FALSE}
params <- c('beta0', 'beta1', 'beta2', 'beta3', 'sigma')

traceplot(bayesion_model, pars = params, inc_warmup = TRUE)
```


```{r}
# launch_shinystan(bayesion_model, rstudio = FALSE)
```


Coeficientes de la regresión múltiple:


```{r}
lineal_model$coefficients
```

Coeficientes descubiertos por la regresión múltiple bayesiana:

```{r}
for(param in params) print(get_posterior_mean(bayesion_model, par=param)[4])
```


```{r}
vars <- c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm') 

models_validation(lineal_model, bayesion_model, params, vars, test_set)
```



## 4. Modelo bayesiano con variables categoricas


## 3. Modelo lineal

```{r}
lineal_model_2 <- lm(
    body_mass_g 
      ~ bill_length_mm
      + bill_depth_mm
      + flipper_length_mm
      + species
      + island
      + sex,

  data = train_set
)

summary(lineal_model_2)
# coefficients_summary(lineal_model)
# anova_summary(lineal_model)
```


Antes que anda transformamos las columnas categoricas a un onehot-encoding:

```{r}
cat_cols      <- train_set %>% cat_col_names()

train_set_cat <- train_set %>% dummify(cat_cols)
test_set_cat  <- test_set  %>% dummify(cat_cols)

train_set_cat
```

```{r}
to_model_input <- function(df) {
  model.matrix(
    body_mass_g 
      ~ bill_length_mm
      + bill_depth_mm
      + flipper_length_mm
    
      + species_adelie
      + species_chinstrap
      + species_gentoo
    
      + island_biscoe
      + island_dream
      + island_torgersen
    
      + sex_female
      + sex_male,
    
    data = df
  )
}

train_X <- train_set_cat %>% to_model_input()
test_X  <- test_set_cat %>% to_model_input()
```


```{r message=FALSE, warning=FALSE}
model_definition <- "
  data {
    int<lower=1>                 obs_count;
    int<lower=1>                 coef_count;
    matrix[obs_count,coef_count] X;
    vector[obs_count]            y;
  }
  parameters {
    vector[coef_count]  beta;
    real<lower=0>       sigma;
  }
  model {
    beta[1] ~ normal(0, 8000);
    
    beta[2] ~ normal(0, 100);
    beta[3] ~ normal(0, 100);
    beta[4] ~ normal(0, 100);

    beta[5] ~ normal(0, 100);
    beta[6] ~ normal(0, 100);
    beta[7] ~ normal(0, 100);

    beta[8] ~ normal(0, 100);
    beta[9] ~ normal(0, 100);
    beta[10] ~ normal(0, 100);

    beta[11] ~ normal(0, 100);
    beta[12] ~ normal(0, 100);

    sigma ~ exponential(0.1);

    y ~ normal(X * beta, sigma);
  }
"

options(mc.cores = 24)

bayesion_model_2 <- stan(
  model_code = model_definition,
  data = list(
      obs_count  = dim(train_X)[1],
      coef_count = dim(train_X)[2],
      y          = colvalues(train_set_cat, 'body_mass_g'),
      X          = train_X
  ),
  chains = 30,
  iter   = 10000,
  warmup = 5000,
  thin   = 1
)
```


```{r}
params2 <- c(
  'beta[1]', 'beta[2]', 'beta[3]', 'beta[4]', 'beta[5]',
  'beta[6]', 'beta[7]', 'beta[8]', 'beta[9]', 'beta[10]', 
  'beta[11]', 'beta[12]', 'sigma'
)

traceplot(bayesion_model_2, pars = params2, inc_warmup = TRUE)
```


```{r}
# launch_shinystan(bayesion_model, rstudio = FALSE)
```


Coeficientes de la regresión múltiple:


```{r}
lineal_model_2$coefficients
```

Coeficientes descubiertos por la regresión múltiple bayesiana:

```{r}
for(param in params2) print(get_posterior_mean(bayesion_model_2, par=param)[4])
```



